# ------------- 1. 导入依赖库（备注：每个库的核心用途）-------------
# numpy：用于所有数值计算（矩阵运算、数组操作等，是深度学习的基础工具）
import numpy as np
# jieba：中文分词库（将连续的中文句子拆成独立的词）
import jieba


# ------------- 2. 定义激活函数及其导数（备注：激活函数的作用是给模型引入非线性，让模型能学习复杂关系）-------------
def tanh(x):
    """
    双曲正切激活函数（隐藏层用）
    作用：将隐藏层输出值映射到 [-1, 1] 之间，引入非线性（没有非线性，RNN和线性模型没区别）
    输入x：任意形状的numpy数组（比如隐藏层的线性计算结果）
    输出：与x同形状的数组，每个元素都在[-1,1]
    提示：直接使用numpy自带的tanh函数即可
    """
    return np.tanh(x)


def tanh_derivative(h):
    """
    tanh函数的导数（反向传播时用）
    作用：计算隐藏层梯度，用于更新权重（反向传播需要链式法则，激活函数的导数是关键环节）
    输入h：tanh函数的输出结果（不是原始x！利用简化公式避免重复计算tanh）
    输出：与h同形状的导数数组
    提示：导数公式可简化为“1减去h的平方”（数学推导结果）
    """
    return 1-h**2


def softmax(x):
    """
    softmax激活函数（输出层用，多分类任务必备）
    作用：将输出层的线性结果（可能是任意实数）映射到 [0, 1] 之间，且所有类别概率和为1（方便判断“属于某类的概率”）
    输入x：形状为(样本数, 类别数)的数组（每个样本对应多个类别的线性得分）
    输出：与x同形状的概率数组（每个元素是该样本属于对应类别的概率）
    关键细节：先减每行最大值防指数溢出（e的大数次方会变成无穷大，导致计算错误），再做后续计算
    提示步骤：
    1. 计算指数：基于“减每行最大值后的结果”求指数
    2. 归一化：每行结果除以该行的指数和，确保概率和为1
    """
    # 步骤1：计算exp（先减每行最大值防溢出）
    exp_scores =np.exp(x-np.max(x,axis=1,keepdims=True))
    # 步骤2：归一化（每行除以该行总和，确保概率和为1）
    return exp_scores/np.sum(exp_scores,axis=1,keepdims=True)


# ------------- 3. 手动实现RNN类（备注：RNN的核心是“隐藏状态”，能记住之前时间步的信息，处理时序数据）-------------
class ManualRNN:
    def __init__(self, input_size, hidden_size, output_size):
        """
        RNN参数初始化（模型的“骨架”搭建，定义权重和偏置——模型的“记忆”和“计算”依赖这些参数）
        参数说明：
        - input_size：输入特征维度（这里是词向量的维度）
        - hidden_size：隐藏层大小（隐藏状态的维度，控制模型“记忆能力”，越大记忆越强但计算越慢）
        - output_size：输出类别数（这里是2类：正面/负面天气）
        权重含义：
        - Wxh：输入层→隐藏层的权重（把词向量“翻译”成隐藏层能处理的格式）
        - Whh：隐藏层→隐藏层的权重（RNN的核心！负责传递上一时间步的“记忆”到当前时间步）
        - Why：隐藏层→输出层的权重（把隐藏层的“记忆”翻译成类别得分）
        偏置含义：
        - bh：隐藏层偏置（给隐藏层线性计算加“偏移量”，避免模型只能学线性关系）
        - by：输出层偏置（同理，给输出层加偏移，增加模型灵活性）
        初始化逻辑：
        1. 权重用小随机数（避免初始权重太大导致梯度爆炸，让模型慢慢学）
        2. 偏置初始化为全0（偏置的作用是“微调”，初始为0不影响，后续训练会自动调整）
        3. 打印权重形状：方便调试，确认维度是否匹配
        4. 初始化缓存字典：存储前向传播的中间结果（反向传播需要这些结果算梯度）
        """
        # 1. 初始化输入→隐藏、隐藏→隐藏、隐藏→输出的权重（小随机数防爆炸）
        self.Wxh =np.random.rand(input_size,hidden_size)
        self.Whh =np.random.rand(hidden_size,hidden_size)
        self.Why =np.random.rand(hidden_size,output_size)
        # 2. 打印权重形状（调试用，确认维度正确）
        print(f"Wxh(输入→隐藏): {self.Wxh.shape}")
        print(f"Whh(隐藏→隐藏): {self.Whh.shape}")
        print(f"Why(隐藏→输出): {self.Why.shape}")

        # 3. 初始化偏置（全0，后续训练调整）
        self.bh =np.zeros((1,hidden_size))
        self.by =np.zeros((1,output_size))

        # 4. 缓存中间结果的字典（反向传播需要前向的隐藏状态、输入等）
        self.cache = {}

    def forward(self, x):
        """
        前向传播（模型“预测”过程：输入词向量序列→输出类别概率，模拟“读句子、做判断”）
        输入x：RNN的标准输入格式，形状(seq_len, batch_size, input_size)
        - seq_len：时间步长（句子的最大词数，长句截断，短句补0）
        - batch_size：一次训练的样本数（提高效率）
        - input_size：词向量维度（每个词用固定维度的向量表示）
        输出y_hat：每个样本的类别概率，形状(batch_size, output_size)
        核心逻辑：
        - 每个时间步更新“隐藏状态”：用当前词向量和上一时间步的记忆，计算当前记忆
        - 最后用“最后一个时间步的记忆”做预测：因为其包含整个句子的信息
        提示步骤：
        1. 解析x的形状：拿到时间步、批量数，后续循环要用
        2. 初始化隐藏状态：第一个时间步没有“之前的记忆”，初始为全0
        3. 创建列表保存每个时间步的隐藏状态（反向传播需要历史状态算梯度）
        4. 循环遍历每个时间步（逐词处理句子）：
           a. 取当前时间步输入（批量内所有样本的该时间步词向量）
           b. 计算当前隐藏状态：当前词信息 + 历史记忆 + 偏置，再经过tanh激活
           c. 保存当前隐藏状态到列表
        5. 计算输出层线性得分：用最后一个时间步的隐藏状态（含全句信息）
        6. 转换线性得分为概率：用softmax确保概率在0-1之间且和为1
        7. 缓存中间结果：输入、隐藏状态、最终隐藏状态、线性得分、概率（供反向传播用）
        8. 返回预测概率（给计算损失和测试用）
        """
        # 1. 解析x的形状（时间步、批量数）
        seq_len, batch_size, _ =x.shape
        # 2. 初始化隐藏状态（全0：第一个词没有历史记忆）
        h =np.zeros((batch_size,self.Whh.shape[0]))
        # 3. 保存每个时间步隐藏状态的列表（反向传播要用）
        hidden_states = []

        # 4. 遍历每个时间步，逐词更新隐藏状态（核心：积累句子信息）
        for t in range(seq_len):
            # a. 当前时间步的输入（批量内所有样本的该时间步词向量）
            x_t =x[t]
            # b. 计算当前隐藏状态（当前词信息 + 历史记忆 + 偏置 → 激活）
            h =tanh(np.dot(x_t,self.Wxh)+np.dot(h,self.Whh)+self.bh)
            # c. 保存当前隐藏状态（供反向传播用）
            hidden_states.append(h)

        # 5. 用最后一个时间步的隐藏状态（全句信息）计算输出层线性得分
        y =np.dot(h,self.Why)+self.by
        # 6. 转换为类别概率（确保概率和为1）
        y_hat =softmax(y)

        # 7. 缓存中间结果（反向传播需要这些值）
        self.cache['x'] =x
        self.cache['hidden_states'] =hidden_states
        self.cache['h_final'] =h
        self.cache['y'] =y
        self.cache['y_hat'] =y_hat

        # 8. 返回预测概率（给计算损失和测试用）
        return

    def backward(self, y_true):
        """
        反向传播（模型“学习”过程：计算每个参数的梯度，告诉模型“权重该怎么调才能预测更准”）
        输入y_true：真实标签（one-hot编码，形状(batch_size, output_size)）
        输出：无返回值，将梯度保存在字典（后续用梯度更新权重）
        核心逻辑：
        - 从输出层往回算（先算输出层参数梯度，再算隐藏层参数梯度）
        - RNN特殊之处：隐藏层参数在每个时间步都被使用，需累加所有时间步的梯度
        提示步骤：
        1. 从缓存取出前向传播的中间结果：输入、所有隐藏状态、最终隐藏状态、预测概率
        2. 解析输入形状：拿到时间步、批量数、输入维度
        3. 初始化所有权重和偏置的梯度：全0（梯度是“调整方向”，初始为0表示未计算）
        4. 计算输出层梯度（输出层参数少，先算）：
           a. 计算输出层线性得分的误差：预测概率与真实标签的差距（交叉熵损失简化推导结果）
           b. 计算隐藏→输出权重的梯度：利用最终隐藏状态和输出误差（链式法则）
           c. 计算输出层偏置的梯度：所有样本的输出误差求和（偏置对每个样本有贡献）
        5. 初始化隐藏层反向误差：将输出层误差传递到隐藏层（通过输出权重反向传播）
        6. 反向遍历时间步（从最后一个到第一个，因隐藏状态依赖前一个时间步记忆）：
           a. 取当前时间步的隐藏状态
           b. 计算当前隐藏状态的梯度：传递来的误差 × tanh导数（链式法则）
           c. 取前一个时间步的隐藏状态（第一个时间步无历史，用全0）
           d. 累加隐藏→隐藏权重的梯度：利用前一时间步隐藏状态和当前梯度（每个时间步都有贡献）
           e. 累加输入→隐藏权重的梯度：利用当前时间步输入和当前梯度（每个时间步都有贡献）
           f. 累加隐藏层偏置的梯度：
               - 原因1：偏置在每个时间步都被使用，需累加所有时间步梯度
               - 原因2：偏置对批量内所有样本有贡献，需先按样本求和
               - 原因3：保持梯度形状与偏置一致，避免更新时形状不匹配
           g. 更新隐藏层反向误差：将当前时间步误差传递到前一个时间步（供计算前一步梯度用）
        7. 梯度平均：除以批量数（避免批量大小影响梯度尺度，确保单样本梯度一致）
        8. 保存所有梯度到字典（供后续参数更新用）
        """
        # 1. 从缓存中取出前向传播的中间结果（反向传播必须用这些值）
        x =self.cache['x']
        hidden_states =self.cache['hidden_states']
        h_final = self.cache['h_final']
        y_hat =self.cache['y_hat']

        # 2. 解析x的形状（获取时间步、批量数、词向量维度）
        seq_len, batch_size, input_size =x.shape

        # 3. 初始化所有参数的梯度（全0，形状和原参数一致）
        dWxh =np.zeros_like(self.Wxh)
        dWhh =np.zeros_like(self.Whh)
        dWhy =np.zeros_like(self.Why)
        dbh =np.zeros_like(self.bh)
        dby =np.zeros_like(self.by)

        # 4. 计算输出层梯度（先算输出层，再反向算隐藏层）
        # a. 输出层线性得分的误差（预测与真实的差距）
        dy =y_true-y_hat
        # b. 隐藏→输出权重的梯度（利用最终隐藏状态和输出误差）
        dWhy =h_final.t@dy
        # c. 输出层偏置的梯度（所有样本误差求和，保持形状）
        dby =dy

        # 5. 初始化隐藏层反向误差（输出层误差传递到隐藏层）
        dh_next =dy@dWhy.t

        # 6. 反向遍历时间步（从最后一个词往第一个词推，计算隐藏层参数梯度）
        for t in reversed(range(seq_len)):
            # a. 当前时间步的隐藏状态（从缓存中取）
            h =hidden_states[t]
            # b. 计算当前隐藏状态的梯度（链式法则：传递误差 × 激活函数导数）
            dh =dh_next@tanh_derivative(h)
            # c. 前一个时间步的隐藏状态（t=0时无历史，用全0）
            if t > 0:
                h_prev =
            else:
                h_prev =
                # d. 累加隐藏→隐藏权重的梯度（每个时间步都用该权重，需累加）
            dWhh +=
            # e. 累加输入→隐藏权重的梯度（每个时间步都用该权重，需累加）
            x_t =
            dWxh +=
            # f. 累加隐藏层偏置的梯度（按原因1-3处理）
            dbh +=
            # g. 更新隐藏层反向误差（传递到前一个时间步，供计算前一步梯度用）
            dh_next =

            # 7. 梯度平均（除以批量大小，避免批量数影响梯度尺度）
        dWxh /=
        dWhh /=
        dWhy /=
        dbh /=
        dby /=

        # 8. 保存所有梯度（给参数更新用）
        self.grads = {
            'dWxh': dWxh,
            'dWhh': dWhh,
            'dWhy': dWhy,
            'dbh': dbh,
            'dby': dby
        }

    def update_parameters(self, learning_rate):
        """
        梯度下降更新参数（用反向传播计算的梯度，调整权重和偏置——模型“纠错”的核心步骤）
        输入learning_rate：学习率（控制每次更新的“步长”，决定参数调整幅度）
        核心公式：新参数 = 旧参数 - 学习率 × 梯度
            - 逻辑：梯度是“参数需要调整的方向”（梯度正→参数太大需减小；梯度负→参数太小需增大）
            - 学习率：避免步长太大导致“震荡不收敛”，或步长太小导致“收敛太慢”
        提示：每个参数（权重、偏置）都按上述核心公式更新
        """
        # 输入→隐藏权重更新
        self.Wxh -=
        # 隐藏→隐藏权重更新（RNN核心记忆参数）
        self.Whh -=
        # 隐藏→输出权重更新
        self.Why -=
        # 隐藏层偏置更新
        self.bh -=
        # 输出层偏置更新
        self.by -=

    # ------------- 4. 辅助函数：计算交叉熵损失（备注：损失函数是“衡量模型预测误差”的工具，误差大则损失大）-------------


def compute_loss(y_hat, y_true):
    """
    计算交叉熵损失（多分类任务的常用损失函数，对概率预测友好，梯度计算简单）
    输入：
    - y_hat：模型预测概率（形状(batch_size, output_size)，每个值0-1）
    - y_true：真实标签（one-hot编码，形状同上，仅正确类别为1）
    输出：平均损失（单个样本的损失值，越小预测越准）
    关键细节：
    1. 加微小值防log(0)（0的对数是负无穷，导致计算错误）
    2. 除以批量数：平均到单个样本（避免批量大小影响损失值，方便对比）
    公式逻辑：-（真实标签 × 预测概率的对数）的总和 / 批量数
        - 逻辑：正确类别预测概率越近1，损失越小；越近0，损失越大
    提示：用numpy的对数函数和求和函数实现公式
    """
    # 步骤1：获取批量大小（用于后续平均）
    batch_size =
    # 步骤2：计算交叉熵损失（加微小值防log(0)，求和后平均到每个样本）
    loss =
    return


# ------------- 5. 核心辅助函数：生成文本样本（备注：将“文字句子”转换成“模型能处理的数值格式”，NLP基础步骤）-------------
def generate_text_data(seq_len=6, embedding_dim=5):
    """
    生成有语义的文本样本（天气句子分类任务），完成5件关键事：
    1. 定义正负样本：明确正面（好天气）、负面（坏天气）句子
    2. 中文分词：把连续句子拆成独立的词
    3. 构建词汇表：给每个不重复词分配唯一索引（方便转数值）
    4. 生成词向量：让同类词向量接近（正面偏正向，负面偏负向），传递语义
    5. 格式转换：输入转RNN要求格式，标签转one-hot（匹配模型输出）

    参数：
    - seq_len：句子最大词数（长句截断，短句补0）
    - embedding_dim：每个词的向量维度（用固定维度向量表示词）

    返回：
    - X：RNN输入序列（模型能直接用的数值格式）
    - y：one-hot标签（匹配模型输出格式）
    - texts：原始句子（调试时查看预测错误的句子）
    - labels：原始标签（0=负面，1=正面，方便计算准确率）
    提示步骤：
    1. 定义正负样本句子：各10个，语义清晰（方便模型学习）
    2. 合并句子和标签：正面句子+负面句子，标签对应为1+0
    3. 中文分词：遍历每个句子，拆分后存入列表，打印结果（调试用）
    4. 构建词汇表：遍历所有分词结果，给不重复词分配唯一索引，打印（调试用）
    5. 生成词向量：
       a. 定义正面词、负面词列表（包含分词后的词，避免语义信号丢失）
       b. 遍历每个词：正面词→正向基础值+小随机数，负面词→负向基础值+小随机数，中性词→接近0的随机数
    6. 转换输入为RNN格式：
       a. 初始化全0数组（短句补0的位置直接为0）
       b. 逐样本、逐时间步填充词向量（不超过最大时间步）
    7. 标签转one-hot：初始化全0数组，给每个样本的正确类别位置设为1
    8. 返回所有需要的变量（给训练和测试用）
    """
    # 1. 定义原始正负样本句子（各10个，语义清晰，方便模型学习）
    positive_texts = [
        # 补全10个正面天气句子
    ]
    negative_texts = [
        # 补全10个负面天气句子
    ]

    # 2. 合并所有句子和标签（正面标1，负面标0）
    texts =
    labels =
    num_samples =

    # 3. 中文分词（拆成词列表），并打印结果（调试用）
    tokenized_texts = []
    for text in texts:
        # 用jieba分词，得到词列表
        words =
        tokenized_texts.append(words)
    print("\n【所有文本样本及分词结果】")
    for i, (text, words) in enumerate(zip(texts, tokenized_texts)):
        print(f"样本{i + 1}: {text} → 分词: {words} → 标签: {'正面天气' if labels[i] == 1 else '负面天气'}")

    # 4. 构建词汇表（词→索引映射），并打印（调试用）
    word_to_idx = {}  # 键：词，值：唯一索引
    idx = 0  # 索引从0开始
    for words in tokenized_texts:
        for word in words:
            if word not in word_to_idx:  # 仅给未出现的词分配索引（避免重复）
                word_to_idx[word] =
                idx += 1
    vocab_size =  # 词汇表大小（不重复词的数量）
    print(f"\n【词汇表】（共{vocab_size}个不重复词）：{word_to_idx}")

    # 5. 生成词向量（让同类词向量接近，传递语义信号——模型区分正负的关键）
    # 初始化词向量矩阵（形状：词汇表大小 × 词向量维度）
    word_vectors =
    # 定义正负词列表（包含分词后的词，避免语义信号丢失）
    positive_words = ["阳光", "阳光明媚", "晴朗", "天气晴朗", "舒服", "惬意", "暖和", "好看", "好", "干爽", "清新",
                      "温暖", "适合", "出游", "散步", "爬山", "晒被子", "花开"]
    negative_words = ["下雨", "大雾", "大风", "暴雨", "寒冷", "冰雹", "阴天", "台风", "沙尘暴", "霜冻", "麻烦",
                      "看不清", "响不停", "积水", "感冒", "砸坏", "压抑", "无聊", "很差", "冻坏"]
    # 给每个词分配向量
    for word, idx in word_to_idx.items():
        if word in positive_words:
            # 正面词：正向基础值 + 小随机数（避免向量重复）
            word_vectors[idx] =
        elif word in negative_words:
            # 负面词：负向基础值 + 小随机数
            word_vectors[idx] =
        else:
            # 中性词：接近0的小随机数（不传递正负语义）
            word_vectors[idx] =

            # 6. 转换输入格式为RNN要求的形状
    # 初始化全0数组（短句补0的位置直接为0）
    X =
    # 逐样本、逐时间步填充词向量
    for sample_idx in range(num_samples):
        # 当前样本的分词结果（词列表）
        words =
        # 遍历每个时间步（不超过最大时间步，避免越界）
        for time_step in range(min(len(words), seq_len)):
            # 当前时间步的词
            word =
            # 查词的索引（从词汇表中获取）
            word_idx =
            # 填充词向量到对应位置
            X[time_step, sample_idx, :] =

            # 7. 标签转one-hot编码（匹配模型输出格式）
    # 初始化全0数组（形状：样本数 × 类别数）
    y =
    # 给每个样本的正确类别位置设为1
    for i, label in enumerate(labels):
        y[i, label] =

        # 8. 返回所有需要的变量（给训练和测试用）
    return X, y, texts, labels


# ------------- 6. 训练函数（备注：整合所有模块，完成“生成数据→训练模型→测试效果”的完整流程）-------------
def train_rnn():
    """
    RNN训练主函数：把前面定义的模块串起来，让模型从“不会”到“会区分正负天气句子”
    核心流程：设置超参数→生成数据→创建模型→批量训练（前向→损失→反向→更新）→测试效果
    提示步骤：
    1. 设置超参数（控制训练过程，根据任务调整）：
       - 时间步、词向量维度、隐藏层大小、输出类别数、学习率、训练轮次、批量大小
    2. 生成文本样本：调用样本生成函数，得到输入、标签、原始句子、原始标签，打印格式（确认合规）
    3. 创建RNN模型：传入输入维度、隐藏层大小、输出类别数
    4. 训练循环（多轮遍历样本）：
       a. 初始化总损失（累计当前轮的损失，后续算平均）
       b. 批量遍历样本（一次取指定数量的样本，提高效率）：
          i. 取当前批量（避免最后一批不足批量大小导致越界）
          ii. 前向传播：模型预测当前批量的概率
          iii. 计算损失：衡量当前批量的预测误差，累加到总损失
          iv. 反向传播：计算当前批量的梯度，告诉模型参数调整方向
          v. 更新参数：用梯度调整权重和偏置，模型“纠错”
       c. 计算平均损失：总损失除以总样本数
       d. 每10轮打印一次平均损失（观察训练进度：损失下降→模型在学习）
    5. 测试模型（用前5个样本验证效果）：
       a. 取前5个样本的输入
       b. 前向传播预测概率
       c. 转换预测概率为标签（取概率最大的类别）
       d. 计算预测置信度（每个样本预测为对应类别的概率）
       e. 打印测试结果表格（样本号、原始句子、真实标签、预测标签、置信度）
       f. 计算测试准确率（正确预测数/总测试数），打印准确率
    """
    # 1. 设置超参数（控制训练过程，根据任务调整）
    seq_len =
    embedding_dim =
    hidden_size =
    output_size =
    learning_rate =
    epochs =
    batch_size =

    # 2. 生成文本样本（把文字转成模型能处理的数值格式）
    X, y, texts, true_labels =
    num_samples =  # 总样本数
    # 打印输入和标签格式（确认是否符合RNN要求）
    print(
        f"\n【模型输入格式】X.shape: {X.shape} → (时间步长={seq_len}, 样本数={num_samples}, 词向量维度={embedding_dim})")
    print(f"【标签格式】y.shape: {y.shape} → (样本数={num_samples}, 类别数={output_size})")

    # 3. 创建RNN模型（初始化权重和偏置，搭建模型骨架）
    rnn =

    # 4. 开始训练循环（多轮遍历样本，让模型逐步学会区分正负天气）
    print("\n" + "=" * 50)
    print("开始训练：学习区分正面天气和负面天气句子")
    for epoch in range(epochs):
        total_loss = 0  # 累计当前轮的总损失

        # 批量遍历样本（一次训指定数量，提高效率）
        for i in range(0, num_samples, batch_size):
            # 取当前批量的样本（避免最后一批越界）
            end_idx =
            batch_X =  # 当前批量的输入
            batch_y =  # 当前批量的标签

            # 前向传播：模型预测当前批量的类别概率
            y_hat =
            # 计算当前批量的损失（衡量预测误差）
            loss =
            # 累加总损失（乘以当前批量样本数，确保总损失是所有样本的和）
            total_loss +=
            # 反向传播：计算梯度，告诉模型参数调整方向
            rnn.backward(batch_y)
            # 更新参数：用梯度调整权重和偏置，模型“纠错”
            rnn.update_parameters(learning_rate)

        # 计算当前轮的平均损失（总损失 / 总样本数）
        avg_loss =
        # 每10轮打印一次损失（观察训练进度）
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch + 1:3d}/{epochs} | 平均损失: {avg_loss:.4f}（损失越小越准）")

    # 5. 测试模型（用前5个样本验证效果）
    print("\n" + "=" * 50)
    print("【模型测试】取前5个样本看预测结果")
    test_idx =  # 测试前5个样本
    test_X =  # 前5个样本的输入

    # 模型预测测试样本的概率
    y_hat =
    # 转换预测概率为标签（取概率最大的类别）
    pred_labels =
    # 计算预测置信度（每个样本预测为对应类别的概率）
    pred_probs =

    # 打印测试结果表格（清晰对比真实值和预测值）
    print(f"{'样本':<4} {'原始句子':<15} {'真实标签':<8} {'预测标签':<8} {'置信度':<6}")
    print("-" * 50)
    for i in range(test_idx):
        # 转换标签为文字（0→负面，1→正面，方便查看）
        true_label =
        pred_label =
        # 打印每一行结果
        print(f"{i + 1:<4} {texts[i]:<15} {true_label:<8} {pred_label:<8} {pred_probs[i]:.4f}")

    # 计算测试准确率（正确预测的样本数 / 总测试样本数）
    accuracy =
    # 打印准确率（1.0表示全对，0.0表示全错）
    print(f"\n测试准确率: {accuracy:.2f}（1.0表示全对，0.0表示全错）")


# ------------- 7. 程序入口（备注：Python规定，只有当脚本直接运行时，才执行训练函数）-------------
if __name__ == "__main__":
    # 调用训练函数，开始整个流程（生成数据→训练→测试）
    train_rnn()